{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 3\n",
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore k-Nearest Neighbors (k-NN) classification and Logistic Regression with Gradient Descent, implementing both models manually to gain an understanding of how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we start by importing the important libararies we will need. Note here we have both random and from collections import counter, both of these are modules included in python itself, but we need to load them explicitly as we would other imported packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#set random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Weeks Dataset: The \"Census Income\" dataset.\n",
    "\n",
    "We are going to focus on the 'Census Income' dataset, from https://archive.ics.uci.edu/dataset/2/adult.\n",
    "\n",
    "The dataset contains demographic and income-related information. The task is to predict whether an individual's income exceeds $50K per year. This is a binary classification problem where the target variable has two possible values: '>50K' and '<=50K'.\n",
    "\n",
    "There are quite a few ways to load in datasets. In this case, because the dataset is available online we can load it in directly using it's url, as shown below (internet access is needed for this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "data = pd.read_csv(url, names=column_names, na_values=' ?', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that whereas last time we imported a csv with ';' as the seperator, this time the file is a .data file, and we manually set column names and tell pandas to read ? values as NA. In general there is no universal rule for how we read in datasets, often it is best to consult the documentation, check code from others who have worked with the same dataset or just try importing, printing and adjusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the functions we have learned or new ones to look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    32561.000000  \n",
       "mean        40.437456  \n",
       "std         12.347429  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we don't have any, let's enjoy this while it lasts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our binary classification tasks we need numerical labels, so lets convert the income category to be binary, one if it is '>50K', and 0 if it is '<=50K'.\n",
    "\n",
    "For this we use the very useful .apply() function which applies a specified function to the rows given. Then we can use use lambda which is a way to define a function in one line. The whole method looks like this: .apply(lambda x: 1 if x == '>50K' else 0). Use this on the 'income' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.income = data.income.apply(lambda x: 1 if x == '>50K' else 0)\n",
    "\n",
    "# if x == '>50k':\n",
    "#     income = 1\n",
    "# else:\n",
    "#     income = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  income  \n",
       "0          2174             0              40  United-States       0  \n",
       "1             0             0              13  United-States       0  \n",
       "2             0             0              40  United-States       0  \n",
       "3             0             0              40  United-States       0  \n",
       "4             0             0              40           Cuba       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to only focus on the numerical features for our applications. We define X and Y as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant numerical features for classification.\n",
    "X = data[['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "y = data['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into test and training sets, with the addition of the validation set.\n",
    "\n",
    " The validation set is what we will use to tune our hyperparameters. It is important we do not use the testing set for this as otherwise we will pick a model due to its strong performance on the testing set and thereby bias or testing error to be lower than it would otherwise be. The test set should stay untouched until the end of our proceedure when we use it to calculate the error!\n",
    "\n",
    " Split your training data twice using train_test_split function first into test and training sets, and then further split the training set into training and validation sets. Use a 60/20/20 split for training/validation/testing.\n",
    "\n",
    " You should end up with 6 dataframes: X_train, X_test, y_train, y_test, X_val and y_val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed) # train is 80 test is 20\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = seed) # train is (3/4)*80=60 val is 1/4*80=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation\n",
    "As we have done before we standardise the data. This helps in improving the performance of models, and is especially important for distance-based methods like k-NN. Remember to fit on the trainnig set and then use this transformation for both validation and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset is very large and our naive implementation of knn is quite computationally expensive, we will create a random subset of the validation set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_subset = random.sample(list(X_val), 100)\n",
    "y_val_subset = random.sample(list(y_val), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to implement the k nearest neighbours algorithm we saw in class.\n",
    "\n",
    "The k-NN algorithm classifies a new data point based on the majority label among its k nearest neighbors. It calculates the distance between the test point and all training points, selects the k closest ones, and assigns the most frequent class label among them.\n",
    "\n",
    "It helps to first define our distance measure as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see if you can implement a knn_predict() function which takes the training set, a test (or validation) set, the training labels and the value of k as input and returns the predicted labels for the test (or validation) set.\n",
    "\n",
    "Hint: here is two lines of code which, if given the indices of the k nearest neighbours will return their most common labels:\n",
    "\n",
    "`k_nearest_labels = [y_train.iloc[i] for i in k_indices] # We get the labels of the k nearest neighbors.`\n",
    "\n",
    "`most_common = Counter(k_nearest_labels).most_common(1)[0][0] # We get the most common class label.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train, X_test, y_train, k):\n",
    "    y_pred = []\n",
    "    for obs in X_test:\n",
    "        distances = [euclidean_distance(obs, X_train[i]) for i in range(len((X_train)))]\n",
    "        ordered_distances = np.argsort(distances)\n",
    "        k_indices = ordered_distances[:k]\n",
    "\n",
    "        k_nearest_labels = [y_train.iloc[i] for i in k_indices] # We get the labels of the k nearest neighbors\n",
    "\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)[0][0] # We get the most common class label\n",
    "\n",
    "        y_pred.append(most_common)\n",
    "\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_predict(X_train, X_val_subset, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function you created on the training data and X_val_subset from above, and then calculate the accuracy of the predicitons, i.e. where your predicted values == X_val_subset. Print this value, and then compute the confusion matrix using the confusion_matrix() function.\n",
    "\n",
    "Note: This code might take a while to run, but if you are getting times above 2 minutes or so something is probably wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nn_accuracy = np.mean(y_pred == y_val_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.77)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_nn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily we don't have to do things with manual implementation! Sklearn has a much more efficient implementation of the KNN classifier. Here we again see the basic structure of fitting models, where we first define the model as an object, then fit it to the data and finally use it to predict over the test (or in this case validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn k-NN Accuracy: 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[74,  8],\n",
       "       [17,  1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_sklearn_knn = knn.predict(X_val_subset)\n",
    "sklearn_accuracy = np.mean(y_pred_sklearn_knn == y_val_subset)\n",
    "print(f'Scikit-learn k-NN Accuracy: {sklearn_accuracy:.2f}')\n",
    "confusion_matrix(y_val_subset, y_pred_sklearn_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use the code above to create a graph of the accuracy of the k-NN classifier for different values of k from 1 to 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 10)\n",
    "accuracies = []\n",
    "\n",
    "for k in range(1, 10):\n",
    "    knn_model = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_knn = knn_model.predict(X_val_subset)\n",
    "    accuracy = np.mean(y_pred_knn == y_val_subset)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('k-NN Classifier Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical model for binary classification. Unlike linear regression, which predicts continuous values, logistic regression predicts probabilities. Central to the model is the sigmoid function which ensures outputs fall between 0 and 1. \n",
    "\n",
    "As logistic regression has no closed form solution (unlike simple linear regression from last week), to train it we need to use gradient descent on the log-loss function (which is specifically called the Binary Cross-Entropy Function when applied to binary classification tasks).\n",
    "\n",
    "We were already introduced to the learning rate in the lectures, which controlls the size of the step, but another important hyperparameter for gradient descent is the number of epochs. This is basically how many steps the gradient descent algorithm should take before it stops. Of course ideally our gradient descent process would converge at a global minimum, and we could simply stop there, but in practice this rarely happens, especially with complex models and as such is best to always set a limit on epochs. Convergence can then be judged by the plotting the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing gradient descent by hand is quite complex, so I have given some example code where you can fill in the code where I have left comments. If you feel very comfortable in python feel free to try to implement it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hints for Implementing Gradient Descent in Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Compute Linear Combination**\n",
    "$$\n",
    "z = X \\theta\n",
    "$$\n",
    "- Use `np.dot()` to compute the dot product.\n",
    "\n",
    "**2. Apply Sigmoid Function**\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**3. Compute Gradient**\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{m} X^T (y_{\\text{pred}} - y)\n",
    "$$\n",
    "\n",
    "**4. Update Parameters**\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "**5. Compute Loss**\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum \\left[ y \\log(y_{\\text{pred}}) + (1 - y) \\log(1 - y_{\\text{pred}}) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define a function for the sigmoid and our cross-entropy loss. Then we implement the gradient descent algorithm. The function should return the parameters theta, the bias therm and the losses at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid function transforms any real number into a probability value between 0 and 1.\n",
    "def sigmoid(z):\n",
    "    result = 1 / (1 + np.exp(-z))\n",
    "    return result\n",
    "\n",
    "# Log-loss function (Binary Cross-Entropy):\n",
    "# It measures how well the predicted probabilities align with the actual labels.\n",
    "def compute_loss(y, y_pred):\n",
    "    result = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) # why does this not work with 1/m ?\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr=0.01, epochs=2000):\n",
    "    m, n = X.shape  # m = number of samples, n = number of features\n",
    "    theta = np.zeros(n)  # Initialize parameters with zeros\n",
    "    losses = []  # Initialize loss history\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over the number of epochs\n",
    "        # Compute linear combination (z = Xθ)\n",
    "        linear_model = np.dot(X, theta)\n",
    "\n",
    "        # Apply sigmoid function\n",
    "        y_pred = sigmoid(linear_model)\n",
    "\n",
    "        # Compute gradient (∇J(θ) = (1/m) * X^T * (y_pred - y))\n",
    "        gradient = (1/m)* np.dot(X.T,(y_pred-y))\n",
    "\n",
    "        # Update all parameters using gradient descent rule (θ = θ - α∇J(θ))\n",
    "        theta = theta - lr * gradient\n",
    "\n",
    "        # Compute loss using the logistic loss function\n",
    "        loss = compute_loss(y, y_pred)\n",
    "        losses.append(loss)  # Save the loss at each epoch\n",
    "\n",
    "        if epoch % 500 == 0:  # Print the loss every 500 epochs\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "    return theta, losses  # Return the learned parameters and the loss history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test if our gradient descent algorithm worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 500, Loss: 0.4739\n",
      "Epoch 1000, Loss: 0.4392\n",
      "Epoch 1500, Loss: 0.4280\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the training data for logistic regression by adding a bias term.\n",
    "X_train_log = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "# Train logistic regression using gradient descent.\n",
    "theta, losses = gradient_descent(X_train_log, y_train.values, lr=0.01, epochs=2000)\n",
    "\n",
    "# Plot loss curve to observe convergence behavior.\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Logistic Regression Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the same code again, but I want you to play around with different values for the learning rate and number of epochs. These are two very important parameters in general, but especially for neural network models so it is good to have an intuitive feeling for them and how they affect convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression using gradient descent.\n",
    "theta, losses = gradient_descent(X_train_log, y_train.values, lr=0.01, epochs=2000)\n",
    "\n",
    "# Plot loss curve to observe convergence behavior.\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Logistic Regression Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a function that takes the parameters 'theta' from our output and calculates the predicted values. To do this it calculates the the linear model for the X values, then runs the results through the sigmoid function to transform them into probabilities, and finally it assigns 1 to all points with a probability over the classification threshold and 0 to all points below it.\n",
    "\n",
    "0.5 is usually used as a claffification threshold but it doesn't have to be, other values can give a more optimum combination of false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print logistic regression accuracy\n",
    "def predict(X, theta, classification_threshold=0.5):\n",
    "    linear_model = np.dot(X, theta)\n",
    "    y_pred = sigmoid(linear_model)\n",
    "    return (y_pred >= classification_threshold).astype(int)\n",
    "\n",
    "X_val_log = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "y_pred_log = predict(X_val_log, theta, classification_threshold=0.5)\n",
    "logistic_accuracy = np.mean(y_pred_log == y_val)\n",
    "print(f'Logistic Regression Accuracy: {logistic_accuracy:.2f}')\n",
    "\n",
    "confusion_matrix(y_val, y_pred_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to know we can use sklearn to do the same thing with much much less code!\n",
    "\n",
    "Note: We don't need to set epochs or learning rate here because sklearn natively uses adaptive and optimized versions of gradient descent which employs second order derivatives and other techniques to converge faster and more accurately. As logistic regression is relatively simpler than other ML methods these algorithms simply run until convergence and will give an error if convergence is not reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_sklearn = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_sklearn)\n",
    "print(f'Logistic Regression Accuracy (Sklearn): {accuracy:.2f}')\n",
    "confusion_matrix(y_val, y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the validation set accuracy, sklearns implementation of logistic regression seems to be the best model. Therefore we fit it on the training and validation data together to use all available data and test it on the test set to get an idea of its generalisation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two lines recombine the training and validation sets to train the model on the full training set:\n",
    "X_train_full = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train_full = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "\n",
    "# Run the model again here and print the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Create a graph which plots the accuracy of the logistic regression classifier for different values of the classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
