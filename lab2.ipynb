{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 2\n",
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Today\n",
    "\n",
    "* [Setup](#setup)\n",
    "* [Dataframes](#dataframes)\n",
    "* [Student Performance Dataset](#dataset)\n",
    "* [Ordinary Least Squares Regression with the Closed Form Solution](#ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup the environment <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2e583-a455-4610-b88f-36f6ad84571e",
   "metadata": {},
   "source": [
    "1. Open Your Terminal or Command Prompt\n",
    "2. Activate Your Conda Environment - make sure it is active before running the installation commands below!\n",
    "   `conda activate ml2025`\n",
    "3. Install Matplotlib\n",
    "   `conda install matplotlib`\n",
    "4. Install statsmodel\n",
    "   `conda install statsmodels`\n",
    "5. Install pandas\n",
    "   `conda install pandas`\n",
    "6. Install sklearn\n",
    "   `conda install scikit-learn`\n",
    "\n",
    "(or `pip install packagename` if you don't use conda or conda install doesn't work for some reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick introduction to the packages we will use:  \n",
    "- **NumPy (`np`)**: NumPy (Numerical Python) provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays. It is highly optimized for numerical computations and serves as the foundation for many other scientific computing libraries, including Pandas. NumPy's `ndarray` is the fundamental data structure, offering efficient operations like element-wise calculations, linear algebra, and statistical functions.  \n",
    "\n",
    "- **Pandas (`pd`)**: Pandas is built on the NumPy package and its key data structure is called the **DataFrame**. DataFrames allow you to store and manipulate tabular data in rows of observations and columns of variables. Pandas provides functions for data cleaning, transformation, and analysis, making it the primary package for handling structured datasets. Unless you are working with large datasets where efficiency is key, this is the main package for loading and managing data.  \n",
    "\n",
    "- **Matplotlib (`plt`)**: Matplotlib is a powerful library for creating static, animated, and interactive visualizations in Python. It provides fine-grained control over plots, allowing users to create line charts, histograms, scatter plots, and more. The `pyplot` module (`plt`) is a commonly used interface that simplifies the process of creating and customizing plots. (Seaborn (`sns`) is a useful library built on top of Matplotlib for even nicer and more complex graphs.)  \n",
    "\n",
    "- **Statsmodels (`sm`)**: Statsmodels is a library for estimating and testing statistical models. It provides tools for performing regression analysis, hypothesis testing, and statistical data exploration. Compared to Scikit-learn, which is optimized for predictive modeling, Statsmodels focuses more on statistical inference, offering detailed output on model parameters, significance tests, and confidence intervals.  \n",
    "\n",
    "- **Scikit-learn (`sklearn`)**: Scikit-learn is one of the most widely used libraries for machine learning in Python. It provides efficient implementations of various machine learning algorithms, including regression, classification, clustering, and dimensionality reduction. In addition to modeling, scikit-learn includes utilities for data preprocessing, model selection, and evaluation, making it a comprehensive tool for building and testing predictive models. As sklearn is a huge package, we usually only import individual modules as we need them.\n",
    "\n",
    "By combining these libraries, we can efficiently analyze data, visualize trends, and build statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of each notebook we must import the necessary libraries for the code to run.\n",
    "\n",
    "You will notice that each of these packages are being installed as a shortened version of their name. This helps keep code short. In practice for common packages this is standardised, and almost everyone will used the same ones which I have included below so it's good to keep them as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86b9fd-5683-4d6c-ae45-aaa00fb52e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have just installed these packages, it's a good practice to verify they has been successfully installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(plt.matplotlib.__version__)\n",
    "print(sm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes with pandas <a class=\"anchor\" id=\"dataframes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pandas cheat sheet to help you with the following excercises: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9382e32-ab59-4d11-9d8c-12d4732f4e51",
   "metadata": {},
   "source": [
    "Let's create a DataFrame from a dictionary. Here, each key becomes a column in the DataFrame, and the values are the data entries for those columns. A dictionary is a built-in data type that stores collections of data as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b3faf-9364-4e4d-8640-c209800b1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 40, 45],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_example)\n",
    "\n",
    "#We can also do this directly in the DataFrame constructor but it's a bit more cumbersome:\n",
    "#df = pd.DataFrame(data_example, columns=['Name', 'Age', 'City'], index=['a', 'b', 'c', 'd', 'e'], City=['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ed5e4-c4ec-4507-8709-c0659a25aa01",
   "metadata": {},
   "source": [
    "Display the data frame to make sure it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef28a25-b5ee-4808-88b1-31ce6419b32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26189145-a3bd-46d7-8b89-635baedba772",
   "metadata": {},
   "source": [
    "Select only the 'Name' and 'Age' Columns and display them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed7c60-561c-4a86-92f0-97f1edbfefb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00510348-6f61-461a-9117-d2639e4fff21",
   "metadata": {},
   "source": [
    "Use .describe() to compute basic statistics for the numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf74f50-4a92-452a-b595-65570ef31aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2badbcfb-31c7-45b6-b467-304739319b74",
   "metadata": {},
   "source": [
    "Subset the DataFrame for people older than 30 and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be273ccf-5b4e-4320-93fe-324425438e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column called 'YearOfBirth' for the year of birth of the people in the dataset (assuming they have already had their birthdays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YearOfBirth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c2d42-13e6-4c56-9e21-8791393d921a",
   "metadata": {},
   "source": [
    "## Student Performance Dataset <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097a948-fc26-4980-b2e1-26b6632ff21f",
   "metadata": {},
   "source": [
    "The dataset used is the student performance dataset as discussed in the lecture this week. More information on the dataset can be found here: https://archive.ics.uci.edu/dataset/320/student+performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to focus on the students maths results rather than their portuguese results.\n",
    "\n",
    "First read in the data from the CSV file into a pandas dataframe using pd.read_csv (the datset is called student-mat.csv):\n",
    "\n",
    "(Hint: you will want to set `\"sep=';'`, as this is the seperator used in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514f93c-01e8-4c96-8827-fa276d5f5f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print the shape of our data to see its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69652c2-2844-490d-8d3f-df4119f736f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84420f37-8b0f-42b4-8c58-0903861af61c",
   "metadata": {},
   "source": [
    "The data has should have 395 samples and 34 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function in pandas for seeing some overview information about datasets is .head(). This prints the first 5 rows of our data. Try it with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782aad4-d9c5-440e-9880-d756c900b9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09091b6-393a-4abf-99c6-ad2a2410e302",
   "metadata": {},
   "source": [
    "#### Checking for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the following code to check for NA values. \n",
    "\n",
    "(There are some subtle differences between `null` which is the default in python, `nan` (Not a Number) from NumPy and the new `pd.NA` value specific to pandas, for our case it's mostly plenty to treat all as missing values, the vast majority you will encounter will be `null`, so that's what we will use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536922f-e411-494d-9dae-6afaea3be735",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_miss = data.columns[data.isnull().any()]\n",
    "\n",
    "print(feat_miss)\n",
    "feat_miss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily in this case we don't have any missing data. We will look at missingness and imputation in future labs however, as most Machine Learning algorithms are not able to natively handle missing values, and many datasets contain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94523934-4628-43d4-ab0a-2b568eaec3e8",
   "metadata": {},
   "source": [
    "### Summary Statistics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see some of the headline information about our data before we analyse it. For this we can use the .describe() function we learned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a500341-5a70-4d2d-a831-5e6927759664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a3c693-8ec8-4687-b239-be4e3ce63094",
   "metadata": {},
   "source": [
    "This gives us the following info:\n",
    "\n",
    "count: The number of non-missing (non-NaN) values.\n",
    "\n",
    "mean: The mean of the values.\n",
    "\n",
    "std: The standard deviation of the values.\n",
    "\n",
    "min: The minimum value.\n",
    "\n",
    "25%: The 25th percentile (first quartile).\n",
    "\n",
    "50% (median): The median of the data.\n",
    "\n",
    "75%: The 75th percentile (third quartile).\n",
    "\n",
    "max: The maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea388b86-ee97-4e17-a68b-9462c480a2d2",
   "metadata": {},
   "source": [
    "We can also specify percentiles other than the default values of 25, 50, 75:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ad2bb-1187-453c-8f2d-e3fdc3925203",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_percentiles = data.describe(percentiles=[.20, .40, .60, .80])\n",
    "print(custom_percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution of the final grade G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6618374-921d-453f-896e-1bf4bfadeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['G3'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above just used a simple line of code, but using matplotlib we can make much more impressive graphs. Below is an example of using matplotlip to create a scatter plot of G1 vs G3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['G1'], data['G3'])\n",
    "plt.xlabel('G1')\n",
    "plt.ylabel('G3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matplotlib to make some more plots of the data to try discover some interesting patterns. See if you can get creative with multiple variables, colours etc.\n",
    "\n",
    "Here is a cheatsheet for matplotlib: https://matplotlib.org/cheatsheets/cheatsheets.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "310b09ac-161a-4087-9211-95b50cc53293",
   "metadata": {},
   "source": [
    "### Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to split our data into seperate dataframes for X and Y. This is common practice as many of the machine learning algorithms we use as well as regularisation and imputation functions will expect the data split up. It is very important to keep track of both and ensure they still match after any preprocessing. If your X values no longer align with your Y targets you will get very wrong results!\n",
    "\n",
    "For this case we want to do a very simple regression so we use the final year grade as our target variable, and first period grade as our only explanatory variable to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['G1'].values\n",
    "y = data['G3'].values\n",
    "\n",
    "#As we have a single feature, we need to reshape it to a 2D array or some of our functions will give errors.\n",
    "X= X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a08c8-af3b-4f6a-9cca-e0a537191973",
   "metadata": {},
   "source": [
    "The **from sklearn.model_selection import train_test_split** command imports the train_test_split function from scikit-learn specifically, which is used to split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb40371-c983-495f-a59d-15bed2faee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Random Seed: **seed = 0** sets the seed for the random number generator to 0. This ensures that the results are reproducible; anyone running this code with the same dataset and seed will get the same split of data. Generally this should be set at the start of your notebook.\n",
    "\n",
    "Splitting the Dataset: The code below splits the features (X) and the target variable (y) into training and test sets. 20% (test_size = 0.2) of the data is allocated to the test set, while the remaining 80% is used for training the model.\n",
    "The random_state = seed parameter ensures that the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffed90c-235d-4db9-98b1-a55fba6f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598586e5-fe9b-400c-a201-7a39ceb45723",
   "metadata": {},
   "source": [
    "### Standardization:\n",
    "Standardization refers to the process of transforming each feature in your data so that it has a mean of 0 and a standard deviation of 1. This is done by subtracting the mean of each feature and then dividing by the standard deviation for each feature. The formula used is:\n",
    "\n",
    "z=(x-μ)/σ\n",
    "Here, x is the original feature value\n",
    "μ is the mean of the feature, and \n",
    "σ is the standard deviation of the feature.\n",
    "\n",
    "This is very useful for machine learning because it can speed up convergence, enhance numerical stability and prevent features from dominating. The main downside is that it makes interpretation harder, but as we are only concerned with prediction that's not an issue for us! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072f6e9-b2f5-4fc8-99c9-94ae5b72591e",
   "metadata": {},
   "source": [
    "To do this follow the following steps:\n",
    "\n",
    "1. Importing StandardScaler:**from sklearn.preprocessing import StandardScaler** imports the StandardScaler class, which provides the functionality to standardize features.\n",
    "\n",
    "2. Creating a StandardScaler Instance: **sc = StandardScaler()** creates an instance of StandardScaler. This instance will then be used to compute the mean and standard deviation for each feature in the dataset, and by keeping the same instance across the testing and training sets we ensure consistancy.\n",
    "   \n",
    "3. Fitting and Transforming the Training Data: **X_train = sc.fit_transform(X_train)** computes the mean and standard deviation of each feature in the training set X_train, and then standardizes the training set by applying the transformation z=(x-μ)/σ. (​The fit_transform method is a combination of fit (to compute the scaling parameters) and transform (to apply the standardization).) The standardized training data is then reassigned to X_train.\n",
    "\n",
    "4. Transforming the Testing Data: **X_test = sc.transform(X_test)** applies the same transformation to X_test using the mean and standard deviation calculated from the training set. It's crucial to use the parameters from the training set to ensure the model evaluates on the same scale. The standardized test data is reassigned to X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e815b0-c432-49a4-8023-78be47f094a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc0b44-1acb-47b7-b366-41b260fb3d48",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression with the Closed Form Solution <a class=\"anchor\" id=\"ols\"></a>:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee41ce-4efa-4393-8407-be24383e33cc",
   "metadata": {},
   "source": [
    "Using the X_train and y_train vectors, calculate the coefficients for a simple OLS Regression by performing the matrix operations for the closed form solution we learned in class.\n",
    "\n",
    "Hint: In order for us to have an intercept in the regression (usually this is something we want), you will have to add a column of ones to X_train. You can do this with the code `X_train = np.hstack((np.ones_like(y_train), X_train))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these coefficients to caclulate the predicted y-values.\n",
    "\n",
    "Hint: You will once again have to add a vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583602f3-d888-4f14-8a9a-ade4eb2cac48",
   "metadata": {},
   "source": [
    "Bonus: It's even better to define a function to do this. Can you make one which takes X_train and y_train as inputs and gives the betas and the predictions as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a769a-7bdf-414d-987e-71ac631a4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c375b1-d0a2-4016-9a39-cc72f9156dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "445a09b2-f72d-40bb-8e71-61774735ce2c",
   "metadata": {},
   "source": [
    "### Calculate Coefficients using Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use statsmodels inbuilt functionalities to calculate fit the model and calculate the predictions. I have included the code here, run it and make sure you understand it as many of the different models we will fit in this course will use similar syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e3416-96da-4a16-8750-f8c15fbb8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also add intercept terms using statsmodels.\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "\n",
    "# Define and fit the model\n",
    "model = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Predictions using statsmodels\n",
    "predictions_statsmodels = model.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f630f65-a423-4838-94ef-081ca05abdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients from closed-form solution:\", beta)\n",
    "print(\"Coefficients from statsmodels:\", model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well they should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1502ae7-f035-4f0f-84a4-60faa8b2ce04",
   "metadata": {},
   "source": [
    "#### Calculate MSE for the Models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the MSE for the models using the mean_squared_error function from sklearn.metrics. It takes the form: `mean_squared_error(y_test, predictions_from_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2da20a-b4ad-44e9-ad4e-42f6068bea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5a249-fcf0-4580-8f5c-7de6a1acd998",
   "metadata": {},
   "source": [
    "### Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create polynomials of X_train and X_test to fit a polynomial regression model. We will use the PolynomialFeatures class from scikit-learn to create the polynomial features. See if you can figure out how to use this function to add the polynomial expansion of our explanatory variable even better if you can make a fucntion to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try create a plot of the mse values for each polynomial degree from 1 to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the polynomial regression model with degree 5, and compare it with the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graphs what do you think the best function for the relationship is? Are our polynomial terms adding value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: If you have time, try add some of the numerical variables we left out of X back in and see if you can make a better function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
