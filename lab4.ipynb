{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 4\n",
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 1 - 1st May – to be rescheduled on the 30th April from 16:00 to 18:00 in room 3.32\n",
    "\n",
    "Lab 2 - 1st May – to be rescheduled on the 30th April from 18:00 to 20:00 in room 3.32\n",
    "\n",
    "Lab 1 - 8th May - to be rescheduled on the 7th May from 16:00 to 18:00 in room 3.32\n",
    "\n",
    "Lab 2 - 8th May - to be rescheduled on the 7th May from 18:00 to 20:00 in room 3.32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we begin by importing the necessary packages and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86b9fd-5683-4d6c-ae45-aaa00fb52e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Performance Dataset <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will once again be working with the student performance dataset this week. More information on the dataset can be found here: https://archive.ics.uci.edu/dataset/320/student+performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('student-mat.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the data to remind yourself of the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test, Training, Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same 60/20/20 split from last lab for training/validation/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffed90c-235d-4db9-98b1-a55fba6f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('G3', axis=1)\n",
    "y = data['G3']\n",
    "\n",
    "#Split the data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *One-hot encoding*\n",
    "As a rule of thumb, the only thing that our models can take as input is columns of numbers. Luckily for us, we can convert almost every other type of data into such columns. In this case, we have quite a few nominal categorical variables, some of which are nominal and others are ordinal.\n",
    "\n",
    "One-hot encoding is a method used to convert categorical variables into a numerical format suitable for machine learning models. Each unique category is represented as a binary vector, where only one element is 1 (indicating the unit is in that category) and the rest are 0. \n",
    "\n",
    "Below is a simple example demonstrating how to use OneHotEncoder() from sklearn.preprocessing to encode a list of animal types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample categorical data\n",
    "animals = np.array([[\"Cat\"], [\"Dog\"], [\"Fish\"], [\"Dog\"], [\"Cat\"]])\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Sparse output = True returns a sparse matrix, while False returns a 2D array\n",
    "\n",
    "# Fit and transform the data\n",
    "onehot_encoded = encoder.fit_transform(animals)\n",
    "\n",
    "# Display results\n",
    "print(\"Categories:\", encoder.categories_)\n",
    "print(\"One-hot encoded representation:\\n\", onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the idea, see if you can create an array of categorical variables such that the one-hot representation looks like:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing Categorical and Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a mix of nominal, ordinal and numerical data we don't want to apply the same transformations to all of them. We can use the ColumnTransformer class to apply the appropriate transformations to each column. Usually we only one-hot encode nominal variables and not ordinal variables as otherwise we could lose information about the ordering. But this is not always the case, sometimes if the distinct categories are more important than the order we might choose to one-hot encode this data also. \n",
    "\n",
    "I would like you to have a look at the data and decide which data we will one-hot encode, which we will call 'one_hot_features', and which we will normalise 'normalise_features'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables:\n",
    "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n",
    "       'Walc', 'health', 'absences', 'G1', 'G2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ##Your code here\n",
    "numerical_features = ##Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the preprocessor and then use it as we would any other transformation, with fit_transform() and transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessors for numerical and categorical data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output= False), categorical_features)\n",
    "])\n",
    "\n",
    "#Use the preprocessor to transform the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc0b44-1acb-47b7-b366-41b260fb3d48",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is similar to linear regression but adds a penalty, called the L2 penalty to the loss function, which discourages large coefficients by penalizing their squared magnitudes. This helps reduce overfitting and multicollinearity by shrinking coefficients toward zero, but it does not set any of them exactly to zero, meaning all features are retained.\n",
    "\n",
    "Now let's get on to fitting the models we learned in class. As you'll notice now that we are done with manual implementation the code is much more compact. This is more what our model fitting will often look like. I would recommend you make sure you understand what each line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a Ridge Regression model\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = ridge.predict(X_train)\n",
    "y_val_pred = ridge.predict(X_val)\n",
    "\n",
    "#Mean Squared Error\n",
    "train_mse = np.mean((y_train - y_train_pred)**2)\n",
    "val_mse = np.mean((y_val - y_val_pred)**2)\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Validation MSE: {val_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec599c4-46b0-485a-93fc-11adcd5359cb",
   "metadata": {},
   "source": [
    "In scikit learn, the lambda of ridge regression is referred to as alpha. Above we defaulted to one as the alpha parameter, which is the default for the Ridge() function. It might not always be the best value however. To find the optimal alpha value we can use our validation set.\n",
    "\n",
    "We want to define a Range of possible Alpha Values to consider. Try to see what np.linspace(-4,4,20) and np.logspace(0, 4, 20) do. Which do you think is more appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ee7de-805b-4288-b80f-7da9b5f8944d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logspace in general is more appropriate for ridge, and quite a few other hyperparameters we work with as scaling the order of magnitude is often more appropriate than scaling linearly. Often the question of what range to use for hyperparameters is an empirical one, which you will build up an intuition for over time, and can also be supplemented by seeing what others have used for similar models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40bdc4-6db9-41c3-aac1-3220cf041bba",
   "metadata": {},
   "source": [
    "Now we want to fit a ridge regression for each of our alpha values. For each value you should fit the model, predict the y values from the validation set and record the MSE and alpha.\n",
    "\n",
    "Hint: Initialize lists of alpha values and mse values outside the loop to store results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388cdfc-636b-4ca8-bb70-fc284c153ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce53235c-87b5-4649-946e-88686e79e295",
   "metadata": {},
   "source": [
    "Use np.argmin on your mse values to find the alpha value that results in the lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfe5d2-f6ba-417a-a7c8-c81f7cc879c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce08fa8-95d9-4556-a88d-05e581086cc6",
   "metadata": {},
   "source": [
    "Plot MSE vs. Alpha\n",
    "Create a plot to visualize how MSE changes with different alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0793ad8-6dd4-4094-a16d-db27b6157e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27b843d3-b419-4c77-832d-5d819d2e6db7",
   "metadata": {},
   "source": [
    "To calculate the final Mean Squared Error (MSE) on the test set using the best alpha value identified from the validation set, we follow the steps ahead. This will give us an estimate of how well your Ridge Regression model is expected to perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49a203-a4a8-4207-baf6-d3b890b64466",
   "metadata": {},
   "source": [
    "Combine Training and Validation Sets\n",
    "For the final model, you'll use both the training and validation data to train it, maximizing the amount of data the model can learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b776ee-4103-468b-a1cd-450c0d5f44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training and validation sets\n",
    "X_train_val = np.vstack((X_train, X_val))\n",
    "y_train_val = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475f607-5333-4b63-a8e2-358e5a9bac98",
   "metadata": {},
   "source": [
    "Using the best alpha value found, train the Ridge Regression model on the combined training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dc160-cf26-42d5-849d-1bf0ac725bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca9a996-ecd4-4231-82e7-79bb79b5d680",
   "metadata": {},
   "source": [
    "Finally, predict on the test set using the final model and calculate the Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899e863-7ffc-4e53-a61e-41018e17e059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8092637e-bf16-4770-82f9-7431ff3e2eb1",
   "metadata": {},
   "source": [
    "What do you conclude from training and test mse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae12e4-73b5-4fbb-9f14-07254774ed63",
   "metadata": {},
   "source": [
    "### Lasso Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f5151-c135-49c0-9ce0-296722382549",
   "metadata": {},
   "source": [
    "Now we move on to Lasso Regression. We use the Lasso(alpha=alpha) function from sklearn to fit lasso regressions. See if you can do the same proceedure as above with Lasso this time. I have given some indications above each block what they shoud do.\n",
    "\n",
    "Initialize lists to store alpha values and their corresponding MSEs. Then, loop over the alpha values, fit a Lasso model for each, and evaluate its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96e0b7-1c61-4a56-a651-c421ee151390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc71575-7624-4ac8-aa6d-4668faef2212",
   "metadata": {},
   "source": [
    "Plot MSE vs. Alpha\n",
    "\n",
    "Visualize how the Mean Squared Error changes with different alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6d5ac-27ff-452b-a6e4-0568bc9a89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a616252-f53c-44e3-8b86-ad808acddf56",
   "metadata": {},
   "source": [
    "Print the Best Alpha and Its MSE\n",
    "Output the best alpha value found and its corresponding lowest MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8316f72-7848-43eb-b009-d4335f642127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b7f9dd1-20ef-4eeb-9f15-f78e5d55ce4f",
   "metadata": {},
   "source": [
    " Combine the training and validation sets for the final model training and fit the model on the combined training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22042bb-0f41-4ba6-adfe-7979fd17a46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Have a look at how many coefficients are non-zero in your Lasso model compared to the ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's fit an LDA model, but as this model is a classifier we need to first transform our label y to be classes. In this case we can use the pass rate to create two categories, making our task to predict if students will pass or fail. We also restrict ourselves to just the previouse test scores for the purposes of easier visualisation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the target to be pass or fail\n",
    "y_LDA = np.where(y > 10, 1, 0)\n",
    "X_LDA = data[['G1', 'G2']]\n",
    "\n",
    "#Now we split the data into training and testing sets\n",
    "X_train_LDA, X_test_LDA, y_train_LDA, y_test_LDA = train_test_split(X_LDA, y_LDA, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement LDA using the LinearDiscriminantAnalysis() function. For the most part the proceedure is the same as fitting a Ridge model above, except with our changed datasets and with accuracy istead of MSE as we are fitting a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Implement LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_LDA, y_train_LDA)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = lda.predict(X_train_LDA)\n",
    "y_test_pred = lda.predict(X_test_LDA)\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy = accuracy_score(y_train_LDA, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test_LDA, y_test_pred)\n",
    "print(\"Train accuracy:\", train_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the same intuition as we saw in class I have plotted the decision boundary below. As we can see the decision boundary is linear as LDA gives Gaussian priors with equal variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid for the two features\n",
    "x_min, x_max = X_LDA.iloc[:, 0].min() - 1, X_LDA.iloc[:, 0].max() + 1\n",
    "y_min, y_max = X_LDA.iloc[:, 1].min() - 1, X_LDA.iloc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict the value of the mesh grid\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_LDA.iloc[:, 0], X_LDA.iloc[:, 1], c=y_LDA, edgecolors='k', s=20)\n",
    "plt.xlabel('G1')\n",
    "plt.ylabel('G2')\n",
    "plt.title('LDA Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Now implement a Logistic Regression and plot the decision boundary for it and compare both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
