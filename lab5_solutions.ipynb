{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 5\n",
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we begin by importing the necessary packages and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86b9fd-5683-4d6c-ae45-aaa00fb52e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Performance Dataset <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will once again be working with the student performance dataset this week. More information on the dataset can be found here: https://archive.ics.uci.edu/dataset/320/student+performance.\n",
    "\n",
    "First load in the dataset and check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('student-mat.csv', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test, Training, Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested still in predicting G3, the students final scores, so this will be our y variable, and all other variables are our X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('G3', axis=1)\n",
    "y = data['G3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "As always we split the data into a training and test set, this time with an 80/20 split. As we are going to use cross-validation we don't have to specify a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffed90c-235d-4db9-98b1-a55fba6f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be doing both regression and classification so we also store the binary variable denoting whether students passed or failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = y_test.apply(lambda x: 1 if x >= 10 else 0)\n",
    "y_train_binary = y_train.apply(lambda x: 1 if x >= 10 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing Categorical and Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a mix of nominal, ordinal and numerical data we don't want to apply the same transformations to all of them. We can use the ColumnTransformer class to apply the appropriate transformations to each column.\n",
    "\n",
    "You do not need to scale variables before using tree-based methods (e.g., decision trees, random forests, gradient boosting methods like XGBoost, LightGBM, and CatBoost). Tree-based models are scale-invariant because they split data based on feature thresholds rather than distance-based calculations. We *can* still scale variables if we want however, and indeed, doing so can have benifits for numerical stability in cases where values get very big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['school', 'sex','address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', \n",
    "                        'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', \n",
    "                        'internet', 'romantic']\n",
    "numerical_features = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', \n",
    "                      'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n",
    "\n",
    "# Preprocessors for numerical and categorical data\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output= False), categorical_features),\n",
    "    ('num', StandardScaler(), numerical_features)\n",
    "])\n",
    "\n",
    "#Use the preprocessor to transform the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc0b44-1acb-47b7-b366-41b260fb3d48",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fitting a very simple decision tree for the binary outcome. You can play around with the max_depth parameter and the plotting to get a sense of how decision trees are formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth = 4,random_state=42)\n",
    "dt_model.fit(X_train, y_train_binary)\n",
    "dt_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the decision tree\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(dt_model, feature_names = preprocessor.get_feature_names_out(), filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we switch tracks to a Decision Tree Regressor. Each leaf node will now contain a continuous value, calculated as the average of the target values in that leaf. For each of the tree-based methods we consider they will have a Classifier and Regressor option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeRegressor(max_depth = 3,random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "dt_reg_pred = dt_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the decision tree\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(dt_reg, feature_names = preprocessor.get_feature_names_out(), filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a method used to evaluate how well a machine learning model performs on unseen data. Instead of using just one training and test split, the data is divided into multiple parts (called \"folds\"). The model is trained on all but one of these folds and tested on the remaining one. This process is repeated multiple times, and the results are averaged to get a more reliable estimate of the model's performance.\n",
    "\n",
    "We use the cross_val_score function for k-fold cross-validation as such: `cross_val_score(model_name, X_train, y_train, cv=k)`. The result will be a score for each fold which we can then average to get our overall estimate for the accuracy.\n",
    "\n",
    "We will use a 10 fold cross validation. In general the larger the dataset, the smaller the fold used as otherwise the fitting process can become much too time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cv_scores = cross_val_score(dt_model, X_train, y_train_binary, cv=10)\n",
    "print(\"Mean 10-Fold CV Score: \", np.mean(dt_cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the test set results and calculating the accuracy, compare the two scores as an indication of how well the cross validation proceedure worked. You can also try changing k above to get an indication of the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Classifier\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test_binary, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important hyperparameter for decision trees is the depth of the tree, which can be set using the max_depth parameter. This cuts the tree off after a certain depth has been reached, which can help prevent overfitting. We can use cross validation the same way we would have used a validation set to pick the optimal value for this hyperparameter.\n",
    "\n",
    "The code below loops over 20 different depths from 1 to 21, calculating both the accuracy on the training set and 10-fold cross-validation accuracy see if you can undertand the code and explain the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(1, 21)\n",
    "dt_train_scores = []\n",
    "dt_cv_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt_model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt_model.fit(X_train, y_train_binary)\n",
    "    dt_train_pred = dt_model.predict(X_train)\n",
    "    dt_train_scores.append(accuracy_score(y_train_binary, dt_train_pred))\n",
    "    dt_cv_scores.append(np.mean(cross_val_score(dt_model, X_train, y_train_binary, cv=10)))\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(depths, dt_train_scores, label='Train')\n",
    "plt.plot(depths, dt_cv_scores, label='CV')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Cross Validation Accuracy Score')\n",
    "plt.title('Decision Tree Accuracy vs Max Depth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest are an ensemble learning model which constructs multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. It achieves robustness by training each tree on a random subset of the data and features, making it effective for both classification and regression tasks.\n",
    "\n",
    "We train Random Forests using the funciton RandomForestRegressor() and RandomForestClassifier(). Fit a regression to our continuose outcome y_train and calculate the MSE and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Predicting the test set results and calculating the accuracy\n",
    "print(\"Random Forest Classifier\")\n",
    "print(\"Test Set MSE: \", mean_squared_error(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea with a Grid Search is that the model tests each differwnt combination of the hyperparameters to find the best-performing combination based on cross-validation. This can be effective for finding optimal models, but very time and computationally expensive.\n",
    "\n",
    "For the random forest Regressor we set up our grid to include the following hyper-parameters:\n",
    "\n",
    "**n_estimators** - This is the number of trees in the forest. A higher number of usually leads to better performance (up to a certain point) but increases computation time.\n",
    "\n",
    "**max_depth** - This is the maximum depth of each tree in the forest. Setting a maximum depth helps control overfitting. Deeper trees capture more patterns but risk overfitting, while shallower trees may underfit. None means nodes are expanded until all leaves are pure or contain fewer than min_samples_split samples.\n",
    "\n",
    "**min_samples_split**: This controls the minimum number of samples required to split a node. A lower value (e.g., 2) allows trees to grow deeper and potentially overfit, while a higher value (e.g., 10) forces trees to be more constrained, reducing overfitting but potentially leading to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "cv_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5) #Here we perform a grid search with 5-fold cross validation\n",
    "cv_rf.fit(X_train, y_train) # Fit the model to the training set - this will perform the grid search and find the model with the best hyper-parameters\n",
    "\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"Mean cross-validated score of the best_estimator:\", cv_rf.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", cv_rf.best_params_)\n",
    "\n",
    "# Refit the model with the best parameters on the entire training set\n",
    "rf_model = cv_rf.best_estimator_\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Calculating the accuracy\n",
    "print(\"Best Random Forest Classifier\")\n",
    "print(\"Test Set MSE: \", mean_squared_error(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a fast, scalable, and regularized gradient boosting algorithm that excels at predictive modeling tasks, especially on structured data. It improves performance through optimized tree learning, parallelization, and built-in regularization to prevent overfitting.\n",
    "\n",
    "See if you can initialise an XGBRegressor and perform a grid search with 5-fold cross validation. I have provided below a param_grid designed for tuning an XGBoost model, specifying a range of values for six key hyperparameters that control its behavior and performance. I have specifically made the list quite short for each hyperparameter to prevent fitting taking impractically long amounts of time.\n",
    "\n",
    "The hyperparameters for XGBoost are as follows:\n",
    "\n",
    "**max_depth:** Sets the maximum depth of each tree. Deeper trees can model more complex relationships but increase the risk of overfitting. Shallower trees improve generalization but may underfit.\n",
    "\n",
    "**min_child_weight:** Defines the minimum sum of instance weights (Hessian) required to create a new node. Higher values make the model more conservative by preventing splits on small, less significant data groups, reducing overfitting.\n",
    "\n",
    "**learning_rate (eta):** Controls the step size in updating weights after each boosting round. Lower values slow down learning but improve generalization, often requiring more trees (n_estimators) to reach optimal performance.\n",
    "\n",
    "**n_estimators:** Specifies the number of boosting rounds (trees) in the ensemble. More trees generally improve performance but can lead to overfitting if not balanced with regularization.\n",
    "\n",
    "**subsample:** Determines the fraction of training samples used in each boosting round. Values below 1.0 introduce randomness, reducing overfitting by making trees less dependent on specific data points.\n",
    "\n",
    "**colsample_bytree:** Defines the fraction of features (columns) randomly selected for each tree. Using fewer features per tree increases diversity in the ensemble, helping prevent overfitting and improving robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [5, 7],  \n",
    "    'min_child_weight': [1, 2, 3],  \n",
    "    'learning_rate': [0.01, 0.05],  \n",
    "    'n_estimators': [50, 100, 150],  \n",
    "    'subsample': [0.75, 0.9],  \n",
    "    'colsample_bytree': [0.75, 0.9]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV and fit to find the best parameters\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Re-train the model with the best parameters on the full training set\n",
    "xgb_optimized = XGBRegressor(**best_params, random_state=42)\n",
    "xgb_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_optimized = xgb_optimized.predict(X_train)\n",
    "\n",
    "# Calculate and print the mean squared error and the root mean squared error on the test set\n",
    "mse = mean_squared_error(y_train, y_pred_optimized)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Mean Squared Error on the Test Set:\", mse)\n",
    "print(\"Root Mean Squared Error on the Test Set:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "A very useful feature of bagging-based tree models is that we can extract feature importances from the trained model.\n",
    "\n",
    "Bonus: See if you can plot the importances which we extract in the  first line below. (Otherwise have a look at the graph in the solutions to see the importances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xgb_optimized.feature_importances_\n",
    "\n",
    "# Hint: We can get feature names from the preprocessor\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Sort feature importances and feature names\n",
    "indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "sorted_importances = importances[indices]\n",
    "sorted_feature_names = feature_names[indices]  # Sort feature names accordingly\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(importances)), sorted_importances, align=\"center\")\n",
    "plt.xticks(range(len(importances)), sorted_feature_names, rotation=90, fontsize=10)  # Rotate for readability\n",
    "plt.xlim([-1, len(importances)])\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM (Light Gradient Boosting Machine) is a fast, efficient gradient boosting algorithm that uses histogram-based learning and leaf-wise tree growth to handle large datasets with lower memory usage. It outperforms traditional boosting methods in speed and accuracy, making it well-suited for high-dimensional structured data tasks.\n",
    "\n",
    "Below is the code for a simple LightGBM model, as well as the most important hyperparameters, see if you can create your own parameter grid and run a cross-validation proceedure.\n",
    "\n",
    "Hint: You can use other models parameters as inspiration, try running some models yourself or turn to the internet for examples! Also: it is not necessary to specify every parameter in your grid, especially at the beginning. There will be a default for each paramter chosen by the package which often works quite well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and use a LightGBM model\n",
    "import lightgbm as lgb\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=feature_names) #For lightgbm, we need to convert the numpy array back to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names) #For lightgbm, we need to convert the numpy array back to a pandas dataframe\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42, verbosity=-1) #We set verbosity to -1 to avoid unnecessary warnings during training\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test)\n",
    "\n",
    "#Predicting the test set results and calculating the accuracy\n",
    "print(\"LightGBM Classifier\")\n",
    "print(\"Test Set MSE: \", mean_squared_error(y_test, lgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree Complexity** `num_leaves`: This parameter controls the maximum number of leaves each tree can have, directly affecting model complexity. A lower value results in simpler trees that generalize well, while a higher value allows for more detailed splits that may lead to overfitting.\n",
    "\n",
    "**Step Size** `learning_rate`: The learning rate determines how much the model adjusts after each boosting iteration. A smaller learning rate slows training but improves generalization, whereas a higher learning rate speeds up convergence at the risk of overfitting.\n",
    "\n",
    "**Number of Trees** `n_estimators`: This defines how many boosting rounds (trees) the model builds to minimize error. More trees generally improve accuracy but increase training time, while too few trees can lead to underfitting.\n",
    "\n",
    "**Row Sampling** `subsample`: This parameter controls what fraction of the training data is randomly selected for each tree. Using a lower value (e.g., 0.8) introduces randomness that prevents overfitting, while using 1.0 means each tree is trained on the full dataset, which can improve performance but also increase overfitting risk.\n",
    "\n",
    "**Feature Sampling** `colsample_bytree`: This parameter sets the fraction of features (columns) that are randomly selected for each tree during training. Lower values introduce diversity and reduce overfitting, while higher values use more features, potentially leading to stronger but less generalizable models.\n",
    "\n",
    "**Minimum Data Per Leaf** `min_child_samples`: The minimum number of data points required in a leaf. Increasing it helps prevent overfitting by forcing larger leaves.\n",
    "\n",
    "**L1 Regularization** `reg_alpha` Adds penalty for large coefficients (sparse solutions). Helps reduce overfitting.\n",
    "\n",
    "**L2 Regularization** `reg_lambda` Similar to L1 but shrinks coefficients more smoothly. Helps control model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your grid search here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
