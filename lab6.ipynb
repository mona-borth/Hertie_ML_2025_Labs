{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 6\n",
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import os\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\" #This is to avoid a memory leakage issue with KMeans on Windows, uncomment if you are on Windows\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finally graduated from using our students dataset, and as recent graduates our first order of business is of course: Wine!\n",
    "\n",
    "This weeks dataset is a quite famous dataset containing the chemical properties of wines derived from three different cultivars grown in the same region in Italy. It consists of 178 samples with 13 numerical features, such as alcohol content, flavonoids, and color intensity.\n",
    "\n",
    "Remember to check out the data for more information here:\n",
    "https://archive.ics.uci.edu/dataset/109/wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X = wine.data  # Feature matrix\n",
    "y = wine.target  # Actual wine classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the X data to a DataFrame as a seperate object and print the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in the lecture, we should scale our X variables, as both PCA and our clustering algorithms are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are doing unsupervised ML we don't need to create a training and test split, as there is no metric we need to calculate or optimise on!\n",
    "\n",
    "Note the dataset does contain a 'true' label, which we will look at, but our goal today is not to recreate this label (a supervised classfication task) but instead unsupervised exploration of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Data\n",
    "To get an overview of the dataset, lets plot two variables at a time, as well as the 'true labels' from the dataset. See if by changing the feature i nthe x and y axis you can get a plot where the same features are roughly in the same location as one another. Which two features work best?\n",
    "\n",
    "Notice we are now using Seaborn (sns) instead of matplotlib,  it is built on top of matplotlib and allows for a more visually appealing plotting. For more information on Seaborn you can visit https://seaborn.pydata.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the features in the dataset\n",
    "print(wine.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x = 0 #Try changing this value to see how the scatter plot changes\n",
    "feature_y = 1 #Try changing this value to see how the scatter plot changes\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_scaled[:, feature_x], y=X_scaled[:, feature_y], hue=y, palette='viridis')\n",
    "plt.title(\"Wine Data\")\n",
    "plt.xlabel(wine.feature_names[feature_x])\n",
    "plt.ylabel(wine.feature_names[feature_y])\n",
    "plt.legend(title=\"Actual Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement PCA, this is quite straightforward in python with the code below. Look at the proportion of variance explained and the graph below, after which component (n) do you think we can sefely drop the rest (i.e. reruning the PCA model with n_components=n)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=13)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot which shows the proportion of variance explained by each component up to the 13th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data as we did above, but this time with the PCA Components as our new features. Which two components give the cleanest split? Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x = 0\n",
    "feature_y = 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, feature_x], y=X_pca[:, feature_y], hue=y, palette='viridis')\n",
    "plt.title(\"Wine Data\")\n",
    "plt.xlabel(\"PCA Component \" + str(feature_x + 1))\n",
    "plt.ylabel(\"PCA Component \" + str(feature_y + 1))\n",
    "plt.legend(title=\"Actual Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rerun the PCA with the number of components you think is optimal. We will then use this data to fit our clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we fit our K-means clustering algorithm. Use the same plot as above with PCA components 1 and 2 as the X and Y values to visualise the clustering. Try changing the number of clusers and see the results. Also try changing the seed (random_state) to see how stable the clustering is at different k values.\n",
    "\n",
    "Hint: Set `hue=clusters`.\n",
    "\n",
    "Bonus: You can also try changing the pca code above and see how different numbers of components in X_pca changes the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is a density-based clustering algorithm that groups points that are closely packed together while marking outliers as noise. Unlike K-Means, DBSCAN does not require you to specify the number of clusters in advance.\n",
    "\n",
    "DBSCAN relies on two parameters:\n",
    "- `epsilon`(ε) – The maximum radius around a point to consider neighbors.\n",
    "- `min_samples` – The minimum number of points required within ε to form a dense region.\n",
    "\n",
    "DBSCAN classifies points into three categories:\n",
    "- **Core Points:** Have at least min_samples points (including itself) within ε.\n",
    "- **Border Points:** Are within ε of a core point but don’t have enough neighbors to be core points themselves. (In our plot below these will be the same as )\n",
    "- **Noise Points:** (Outliers): Are neither core nor border points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are getting strange results, try changing the PCA components\n",
    "#pca = PCA(n_components=2)\n",
    "#X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the DBSCAN algorithm here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot your clusters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Your Own Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can implement another clustering algorithm either from the options we saw in class or any other algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score is one metric used to guage the quality of a clustering algorithm. It measures how well-separated the clusters are and how similar points within the same cluster are to each other. It ranges from 0 to 1, with higher meaning the clusters are more seperated.\n",
    "\n",
    "BUT we should keep in mind that T=the silhouette score generally decreases as the number of clusters increases because smaller clusters tend to have higher compactness but may be closer to other clusters, so we cannot always make direct comparisons. And a higher silhouette score doesn't always mean a \"better\" number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the silhouette score for KMeans and DBSCAN\n",
    "silhouette_kmeans = silhouette_score(X_pca, clusters)\n",
    "silhouette_dbscan = silhouette_score(X_pca, clusters_db)\n",
    "#calculate silhouette score for your own clustering algorithm here\n",
    "\n",
    "print(\"Silhouette Score for KMeans:\", silhouette_kmeans)\n",
    "print(\"Silhouette Score for DBSCAN:\", silhouette_dbscan)\n",
    "#Print the silhouette score for your own clustering algorithm here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Clustering is an unsuppervised method, there is no objective metric by which we can decide how well it has done. Often we use PCA and clustering as part of a semi-supervised process where we may eventually have a metric to give us insight. For 2D and 3D data a visual inspection is valid to decide  which algorithm is best. It is als overy important to consider what you actually want from the process and how your data is structured. Why is it important to find clusters in the first place, and on what criteria are we considering points to be well grouped. Consider these questions and look at our graphs to decide how you think the wines are best seperated!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
